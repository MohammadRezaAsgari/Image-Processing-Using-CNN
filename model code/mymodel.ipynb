{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img_pil = Image.fromarray((img[0, 0] * 255).astype(np.uint8), mode='L')\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_class():\n",
    "    directory_path = os.getcwd()\n",
    "    files = os.listdir(directory_path + '/dataset')\n",
    "    image_files = sorted([file for file in files if file.lower().endswith(('.jpeg'))], key=lambda x: int(x.split('.')[0]))\n",
    "    full_paths = [os.path.join(directory_path + '/dataset/', file) for file in image_files]\n",
    "\n",
    "    classes = []\n",
    "    classes.extend([\"chavoshi\"] * 20)\n",
    "    classes.extend([\"shajarian\"] * 23)\n",
    "    classes.extend([\"khaliq\"] * 21)\n",
    "    classes.extend([\"radan\"] * 25)\n",
    "    classes.extend([\"bayati\"] * 21)\n",
    "    classes.extend([\"kianafshar\"] * 25)\n",
    "    classes.extend([\"alidoosti\"] * 27)\n",
    "    classes.extend([\"qaforian\"] * 25)\n",
    "    classes.extend([\"razavian\"] * 20)\n",
    "    classes.extend([\"daei\"] * 27)\n",
    "    classes.extend([\"attaran\"] * 45)\n",
    "    classes.extend([\"beiranvand\"] * 32)\n",
    "    classes.extend([\"dolatshahi\"] * 24)\n",
    "    classes.extend([\"esfahani\"] * 25)\n",
    "    classes.extend([\"hoceini\"] * 20)\n",
    " \n",
    "    return full_paths,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths,labels = get_name_and_class()\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(image_paths, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {\n",
    "    \"chavoshi\": 0,\n",
    "    \"shajarian\": 1,\n",
    "    \"khaliq\": 2,\n",
    "    \"radan\": 3,\n",
    "    \"bayati\": 4,\n",
    "    \"kianafshar\": 5,\n",
    "    \"alidoosti\": 6,\n",
    "    \"qaforian\": 7,\n",
    "    \"razavian\": 8,\n",
    "    \"daei\": 9,\n",
    "    \"attaran\": 10,\n",
    "    \"beiranvand\": 11,\n",
    "    \"dolatshahi\": 12,\n",
    "    \"esfahani\": 13,\n",
    "    \"hoceini\": 14,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = [class_to_label[label] for label in labels]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess_image(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long) \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = FaceDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = FaceDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = FaceDataset(X_test, y_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FaceRecognitionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 64 * 64)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceRecognitionModel(num_classes=len(set(labels)))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roham\\AppData\\Local\\Temp\\ipykernel_2992\\778674191.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "C:\\Users\\Roham\\AppData\\Local\\Temp\\ipykernel_2992\\778674191.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Validation Loss: 3.1384 | Validation Accuracy: 6.58%\n",
      "Epoch 2/10 | Validation Loss: 2.7253 | Validation Accuracy: 9.21%\n",
      "Epoch 3/10 | Validation Loss: 2.7057 | Validation Accuracy: 3.95%\n",
      "Epoch 4/10 | Validation Loss: 2.6883 | Validation Accuracy: 13.16%\n",
      "Epoch 5/10 | Validation Loss: 2.7000 | Validation Accuracy: 19.74%\n",
      "Epoch 6/10 | Validation Loss: 2.7319 | Validation Accuracy: 11.84%\n",
      "Epoch 7/10 | Validation Loss: 2.8547 | Validation Accuracy: 14.47%\n",
      "Epoch 8/10 | Validation Loss: 2.9872 | Validation Accuracy: 17.11%\n",
      "Epoch 9/10 | Validation Loss: 2.8624 | Validation Accuracy: 17.11%\n",
      "Epoch 10/10 | Validation Loss: 3.1497 | Validation Accuracy: 15.79%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Validation Loss: {average_val_loss:.4f} | Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"face_recognition_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceRecognitionModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=262144, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=15, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"face_recognition_model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 17.11%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
