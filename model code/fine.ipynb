{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Convert grayscale image to 3 channels\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    img_rgb = cv2.resize(img_rgb, target_size)\n",
    "    img_rgb = img_rgb / 255.0\n",
    "    img_pil = Image.fromarray((img_rgb * 255).astype(np.uint8))\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_class():\n",
    "    directory_path = os.getcwd()\n",
    "    files = os.listdir(directory_path + '/dataset')\n",
    "    image_files = sorted([file for file in files if file.lower().endswith(('.jpeg'))], key=lambda x: int(x.split('.')[0]))\n",
    "    full_paths = [os.path.join(directory_path + '/dataset/', file) for file in image_files]\n",
    "\n",
    "    classes = []\n",
    "    classes.extend([\"chavoshi\"] * 20)\n",
    "    classes.extend([\"shajarian\"] * 23)\n",
    "    classes.extend([\"khaliq\"] * 21)\n",
    "    classes.extend([\"radan\"] * 25)\n",
    "    classes.extend([\"bayati\"] * 21)\n",
    "    classes.extend([\"kianafshar\"] * 25)\n",
    "    classes.extend([\"alidoosti\"] * 27)\n",
    "    classes.extend([\"qaforian\"] * 25)\n",
    "    classes.extend([\"razavian\"] * 20)\n",
    "    classes.extend([\"daei\"] * 27)\n",
    "    classes.extend([\"attaran\"] * 45)\n",
    "    classes.extend([\"beiranvand\"] * 32)\n",
    "    classes.extend([\"dolatshahi\"] * 24)\n",
    "    classes.extend([\"esfahani\"] * 25)\n",
    "    classes.extend([\"hoceini\"] * 20)\n",
    " \n",
    "    return full_paths,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths,labels = get_name_and_class()\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(image_paths, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {\n",
    "    \"chavoshi\": 0,\n",
    "    \"shajarian\": 1,\n",
    "    \"khaliq\": 2,\n",
    "    \"radan\": 3,\n",
    "    \"bayati\": 4,\n",
    "    \"kianafshar\": 5,\n",
    "    \"alidoosti\": 6,\n",
    "    \"qaforian\": 7,\n",
    "    \"razavian\": 8,\n",
    "    \"daei\": 9,\n",
    "    \"attaran\": 10,\n",
    "    \"beiranvand\": 11,\n",
    "    \"dolatshahi\": 12,\n",
    "    \"esfahani\": 13,\n",
    "    \"hoceini\": 14,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = [class_to_label[label] for label in labels]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess_image(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long) \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = FaceDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = FaceDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = FaceDataset(X_test, y_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FaceRecognitionModel, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet-18 model\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        # Remove the last fully connected layer\n",
    "        self.resnet_layers = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "        # Add your custom layers with increased capacity\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(1024)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through ResNet layers\n",
    "        x = self.resnet_layers(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.relu(self.batch_norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batch_norm2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roham\\Desktop\\ml-project\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Roham\\Desktop\\ml-project\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = FaceRecognitionModel(num_classes=len(set(labels)))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=8, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roham\\AppData\\Local\\Temp\\ipykernel_15424\\2654636550.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n",
      "c:\\Users\\Roham\\Desktop\\ml-project\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "C:\\Users\\Roham\\AppData\\Local\\Temp\\ipykernel_15424\\2654636550.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | Validation Loss: 2.5018 | Validation Accuracy: 21.05%\n",
      "Epoch 2/12 | Validation Loss: 2.1931 | Validation Accuracy: 25.00%\n",
      "Epoch 3/12 | Validation Loss: 1.9370 | Validation Accuracy: 34.21%\n",
      "Epoch 4/12 | Validation Loss: 1.7704 | Validation Accuracy: 44.74%\n",
      "Epoch 5/12 | Validation Loss: 1.8624 | Validation Accuracy: 52.63%\n",
      "Epoch 6/12 | Validation Loss: 2.0851 | Validation Accuracy: 38.16%\n",
      "Epoch 7/12 | Validation Loss: 2.0633 | Validation Accuracy: 40.79%\n",
      "Epoch 8/12 | Validation Loss: 1.4420 | Validation Accuracy: 60.53%\n",
      "Epoch 9/12 | Validation Loss: 1.4149 | Validation Accuracy: 60.53%\n",
      "Epoch 10/12 | Validation Loss: 1.6593 | Validation Accuracy: 53.95%\n",
      "Epoch 11/12 | Validation Loss: 1.6303 | Validation Accuracy: 51.32%\n",
      "Epoch 12/12 | Validation Loss: 1.7780 | Validation Accuracy: 43.42%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(loss)  # Pass loss to scheduler for dynamic adjustment\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Validation Loss: {average_val_loss:.4f} | Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"fine_face_recognition_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"fine_face_recognition_model.pth\"))\n",
    "evaled = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roham\\Desktop\\ml-project\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Roham\\Desktop\\ml-project\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = FaceRecognitionModel(num_classes=15)\n",
    "model.load_state_dict(torch.load(\"fine_face_recognition_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Define the transform for preprocessing the input image\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def predict_class(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example usage\n",
    "image_paths_to_predict = [\"test.jpeg\", ]\n",
    "exam_labels = [1,]\n",
    "correct = 0\n",
    "for i,image in enumerate(image_paths_to_predict):\n",
    "    predicted_class = predict_class(image)\n",
    "    if predict_class==exam_labels[i]:\n",
    "        correct += 1\n",
    "    # # Map the predicted class index to the corresponding label\n",
    "    # index_to_class = {v: k for k, v in class_to_label.items()}\n",
    "    \n",
    "    # predicted_label = index_to_class[predicted_class]\n",
    "    # print(f\"Predicted Class: {predicted_label}\",)\n",
    "accuracy = correct / len(image_paths_to_predict)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
